name: "llama2_7b"
platform: "pytorch_libtorch"
max_batch_size: 8
dynamic_batching {
    max_queue_delay_microseconds: 100000
    preferred_batch_size: [1, 2, 4, 8]
}
input [
  {
    name: "input_ids"
    data_type: TYPE_INT64
    dims: [-1, -1]
  },
  {
    name: "attention_mask"
    data_type: TYPE_INT64
    dims: [-1, -1]
  }
]
output [
  {
    name: "output"
    data_type: TYPE_FP32
    dims: [-1, -1, 4096]
  }
]
instance_group [
  {
    kind: KIND_GPU
    count: 1
    gpus: [0]
  }
]
parameters: {
  key: "model_type"
  value: {
    string_value: "llama"
  }
}