#!/bin/bash
set -e

echo ">>> 1. IGNORING LOCAL FILES: FORCING RAW JSON OVERWRITE IN CLUSTER..."
# We use jq to forcefully overwrite the storageInitializer block with 100% valid K8s resources
kubectl get cm inferenceservice-config -n kserve -o json | \
  jq '.data.storageInitializer = "{\n  \"image\": \"kserve/storage-initializer:v0.14.0\",\n  \"cpuRequest\": \"100m\",\n  \"cpuLimit\": \"1\",\n  \"memoryRequest\": \"200Mi\",\n  \"memoryLimit\": \"1Gi\"\n}"' | \
  kubectl apply -f -

echo ">>> 2. HARD-KILLING KSERVE CONTROLLER (DUMPING CACHE)..."
kubectl delete pod -n kserve -l control-plane=kserve-controller-manager
sleep 5
kubectl wait --for=condition=Ready pod -n kserve -l control-plane=kserve-controller-manager --timeout=120s

echo ">>> 3. VAPORIZING THE BROKEN MODEL..."
kubectl delete isvc sklearn-iris -n mlops-demo --wait=false --ignore-not-found || true
kubectl patch isvc sklearn-iris -n mlops-demo --type=json -p='[{"op": "remove", "path": "/metadata/finalizers"}]' 2>/dev/null || true
sleep 5

echo ">>> 4. DEPLOYING FRESH RAW MODEL..."
cat <<EOF | kubectl apply -f -
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: sklearn-iris
  namespace: mlops-demo
  annotations:
    serving.kserve.io/deploymentMode: "RawDeployment"
spec:
  predictor:
    model:
      modelFormat:
        name: sklearn
      storageUri: "gs://kfserving-examples/models/sklearn/1.0/model"
EOF

echo ">>> 5. WAITING FOR DEPLOYMENT GENERATION..."
while ! kubectl get deployment sklearn-iris-predictor-default -n mlops-demo > /dev/null 2>&1; do 
    echo -n "."
    sleep 2
done
echo -e "\nDeployment successfully generated by KServe!"

echo ">>> 6. WAITING FOR PODS TO SPIN UP..."
kubectl wait --for=condition=Available deployment/sklearn-iris-predictor-default -n mlops-demo --timeout=120s

echo ">>> 7. TESTING THE INFERENCE ENDPOINT..."
SERVICE_URL=$(kubectl get inferenceservice sklearn-iris -n mlops-demo -o jsonpath='{.status.url}')
if [ -z "$SERVICE_URL" ]; then
  echo "Error: URL not provisioned."
  exit 1
fi

echo "Endpoint is live at: $SERVICE_URL"
curl -s -w "\nHTTP Status: %{http_code}\n" -H "Content-Type: application/json" \
  "${SERVICE_URL}/v1/models/sklearn-iris:predict" \
  -d '{"instances": [[6.8, 2.8, 4.8, 1.4], [6.0, 3.4, 4.5, 1.6]]}'

echo -e "\nâœ… BOOM! Your MLOps platform is officially routing inference traffic."